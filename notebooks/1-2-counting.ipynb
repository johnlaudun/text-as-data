{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Counting Words, er, Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand what it means to tokenize a string into a list of words, we want to explore the various ways we can do that. While it seem strange not to use an already available function like `nltk.tokenize.word_tokenize`, there will be moments in which you would like to have more control of how strings get split in lists of tokens. E.g., I often prefer to keep contractions together in my analyses, and so I prefer to tokenize using a `regex` expression. As we will see both here in the third section which uses functionality available in **pandas**, there are a number of libraries that handle tokenization. Having a better idea of how this works gives you more control of the process which allows you more say in the nature of the data.\n",
    "\n",
    "There are many ways to create a frequency list. Three are below: using built-in functions, using the `nltk`, and using `pandas`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> In each of the sections that follow, I import the libraries as needed. In general, I will develop notebooks in this fashion, but as I continue to work, I tend to migrate all imports to the top of the notebook so that when someone else uses the notebook they can see at a glance what libraries they will need. \n",
    "</div>\n",
    "\n",
    "*If you want to know how to get these colored \"callout\" boxes, see [IBM's markdown guide](https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "\n",
    "# Open and read the file to create a string object\n",
    "with open(\"../data/mdg.txt\", mode=\"r\") as f:\n",
    "    mdg_string = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mdg_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten = str(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "print(10, ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "We have replaced the simple but terrible <code>open</code> above with a much more Pythonic <code>with</code> statement, which has the advantage of automatically closing the file once you have read it into an object.</div>\n",
    "\n",
    "We are creating only one `string` object which we will then tokenize using the three approaches below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, let's check to make sure that the object we created above is as expected. In the previous notebooks, I often called both the `type()` and `print()` functions, but in practice I typically simply use the latter: if something doesn't print or prints in an unexpected fashion, that tells me I need to take a look at my code.\n",
    "\n",
    "Note that I am using the slice method on a string -- you see this far more often with lists, but it works here to limit the amount of text returned. We don't need to see all the characters, just some number of them. Since plain text files conventionally have line lengths of either 72 or 80 columns (or characters), I just choose the arbitrary number of 50 so the output fits on one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Off there to the right -- somewhere -- is a large island,\" said Whitney. \"It's rather a mystery -- \n"
     ]
    }
   ],
   "source": [
    "# Check our work\n",
    "print(mdg_string[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Built-In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import re` tells Python to import the **regex** library, which comes standard with Python itself. (There are a number of default modules: check the documentation to see just how much you get with just the base install!)\n",
    "\n",
    "**regex* is short for *regular expressions, which is as close to voodoo as you can get. No matter what programming language you use, if you deal with texts, you will inevitably have to work up some regular expression or another. It is not easy. (I have two books on it, and I still use sites like [regexr](https://regexr.com)) to figure out what I'm doing. I'm told a number of the LLMs are pretty good at it. Try any/all approaches. \n",
    "\n",
    "Let's first try this code and then we will unpack it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['off', 'there', 'to', 'the', 'right', 'somewhere', 'is', 'a', 'large', 'island', 'said', 'whitney', \"it's\", 'rather', 'a', 'mystery', 'what', 'island', 'is', 'it', 'rainsford', 'asked', 'the', 'old', 'charts', 'call', 'it', \"'ship\", 'trap', 'island', \"'\", 'whitney', 'replied', 'a', 'suggestive', 'name', \"isn't\", 'it', 'sailors', 'have', 'a', 'curious', 'dread', 'of', 'the', 'place', 'i', \"don't\", 'know', 'why']\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "\n",
    "# Let regex do the heavy lifting:\n",
    "mdg_words = re.sub(\"[^a-zA-Z']\",\" \", mdg_string).lower().split()\n",
    "\n",
    "# Check our work\n",
    "print(type(mdg_words))\n",
    "print(mdg_words[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all code blocks are going to be explained, but it might be useful to understand what is happening with `re.sub()` above. *There's a lot happening there!*\n",
    "\n",
    "First, you need to remember that all libraries come with a variety of functions -- that's what makes them libraries! When you import a library, you import all its functions, which are themselves contained within its *name space*. That means you can't simply call a function without first telling Python what library it is found within. Thus, if we want to use regex to **substitute** certain characters for other characters, we have to call the corresponding function. \n",
    "\n",
    "`re.sub()` takes three arguments: what you want it to find, what you want to substitute, and what object you want it to do this to. In the line above, we are telling `re.sub()` that we want to find all instances of characters that are not lowercase letters and not uppercase letters and not an apostrophe, that we want to substitute those non-letters and apostrophes for a space, and that we want to do this to the mdg_string object.\n",
    "\n",
    "Having done that, we then simply lowercase everything and then split on the spaces. *Ta da!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['off', 'there', 'to', 'the', 'right']\n"
     ]
    }
   ],
   "source": [
    "# Check our results:\n",
    "print(type(mdg_words))\n",
    "print(mdg_words[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8017\n"
     ]
    }
   ],
   "source": [
    "print(len(mdg_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we have words or, rather, tokens. Now we would like to go through the text to see how many of each of those tokens we have. The easiest way to do that is to ask Python to create a dictionary, and then to add 1 to its count if the word is already in the dictionary or to start a count of 1 if it is not. (And that is exactly what the `for` loop below does.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdg_dict is a <class 'dict'> of 1947 entries (tokens).\n"
     ]
    }
   ],
   "source": [
    "# Using a dictionary\n",
    "mdg_dict = {}\n",
    "for word in mdg_words:\n",
    "    try:\n",
    "        mdg_dict[word] += 1\n",
    "    except: \n",
    "        mdg_dict[word] = 1\n",
    "\n",
    "# When in doubt print something out\n",
    "print(f\"mdg_dict is a {type(mdg_dict)} of {len(mdg_dict)} entries (tokens).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries are not slice-able like lists, so if we wanted to see inside the dictionary `print(mdg_dict[0:10])` would not work. Dictionaries are made up of key, valye pairs and look like this:\n",
    "\n",
    "a_dictionary { \"key1\":\"value1\", \"key2\", \"value2\"}\n",
    "\n",
    "- Strings are indicated by quotation marks, single **' '** or double **\" \"**.\n",
    "- Lists are indicated by square brackets **[ ]**.\n",
    "- Dictionaries are indicated by curly braces **{ }**.\n",
    "\n",
    "We have already seen these data types, the other oft-used basic data structure in Python, as least in this line of work, is the tuple, whose key feature is that, unlike strings, dictionaries, and lists, it is immutable. (You will want to look that up to understand what that means.) Of course, as you have already begun to understand, all of these data structures, or objects, can contain other data structures within them. So far we have created strings from text files and then we have created lists of strings, or substrings if you prefer, from those strings. Now you are seeing dictionaries using strings for keys and integers for values.\n",
    "\n",
    "*Oh, yes, **integers** and **floats** are also object types in Python, along with Booleans (**`True`** and **`False`**).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To retrieve a value, enter the key:\n",
    "mdg_dict[\"hunter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Substitute other words from \"The Most Dangerous Game\" in the code above to see their counts. Try, for example, \"the.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mystery 1\n",
      "charts 1\n",
      "'ship 1\n",
      "replied 1\n",
      "suggestive 1\n",
      "superstition 1\n",
      "peer 1\n",
      "dank 1\n",
      "tropical 1\n",
      "palpable 1\n",
      "pick 1\n",
      "miles 1\n",
      "moonless 1\n",
      "admitted 1\n",
      "ugh 1\n",
      "moist 1\n",
      "velvet 1\n",
      "rio 1\n",
      "promised 1\n",
      "few 1\n",
      "purdey's 1\n",
      "agreed 1\n",
      "amended 1\n",
      "rot 1\n",
      "philosopher 1\n",
      "cares 1\n",
      "bah 1\n",
      "they've 1\n",
      "understanding 1\n",
      "understand 1\n",
      "nonsense 1\n",
      "hot 1\n",
      "weather 1\n",
      "realist 1\n",
      "classes 1\n",
      "huntees 1\n",
      "we've 1\n",
      "passed 1\n",
      "reputation 1\n",
      "wouldn't 1\n",
      "forsaken 1\n",
      "gotten 1\n",
      "somehow 1\n",
      "didn't 1\n",
      "notice 1\n",
      "crew's 1\n",
      "nerves 1\n",
      "jumpy 1\n",
      "strange 1\n",
      "mention 1\n",
      "captain 1\n",
      "nielsen 1\n",
      "tough 1\n",
      "minded 1\n",
      "swede 1\n",
      "who'd 1\n",
      "ask 1\n",
      "fishy 1\n",
      "among 1\n",
      "seafaring 1\n",
      "gravely 1\n",
      "anything 1\n",
      "actually 1\n",
      "poisonous 1\n",
      "mustn't 1\n",
      "drawing 1\n",
      "mental 1\n",
      "pure 1\n",
      "imagination 1\n",
      "superstitious 1\n",
      "taint 1\n",
      "ship's 1\n",
      "company 1\n",
      "maybe 1\n",
      "extra 1\n",
      "sense 1\n",
      "tells 1\n",
      "wave 1\n",
      "lengths 1\n",
      "broadcast 1\n"
     ]
    }
   ],
   "source": [
    "# Knowing this, you can actually get the most frequent tokens. \n",
    "# What would you change to get the least frequent tokens?\n",
    "for word in sorted(mdg_dict, key=mdg_dict.get, reverse=False)[0:80]:\n",
    "    print(word, mdg_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "wordcloud = wordcloud.WordCloud().generate(mdg_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# # lower max_font_size\n",
    "# plt.figure()\n",
    "# plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# The pil way (if you don't have matplotlib)\n",
    "image = wordcloud.to_image()\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "This is my preferred way, for a variety of reasons, and I'm going to show it to you quickly and without much explanation just for reference before proceeding to walk through the `nltk` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Then use my preferred way to turn a string of words into a list of words\n",
    "words = re.sub(\"[^a-zA-Z']\",\" \", mdg_string).lower().split()\n",
    "\n",
    "# Then create a pandas series\n",
    "mdg_series = pd.Series(words)\n",
    "\n",
    "# pandas series are a particular data structure\n",
    "mdg_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_counts = mdg_series.value_counts()\n",
    "print(mdg_counts[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the niceties of working with pandas\n",
    "mdg_counts.to_csv(\"mdg-counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas makes certain kinds of graphing very easy\n",
    "mdg_counts.iloc[0:99].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways forward: using only NLTK functions or, in the second cell below, combining using preferred word list with the NLTK's `FreqDist` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Off there to the right -- somewhere -- is a large island,\" said Whitney.\n",
      "\"It's rather a mystery -- \"\n",
      "\"What island is it?\"\n",
      "Rainsford asked.\n",
      "\"The old charts call it 'Ship-Trap Island,'\" Whitney replied.\n",
      "\"A suggestive name, isn't it?\n"
     ]
    }
   ],
   "source": [
    "# Testing out sentence tokenizing:\n",
    "mdg_sentences = nltk.tokenize.sent_tokenize(mdg_string)\n",
    "\n",
    "# Let's see some results:\n",
    "for sentence in mdg_sentences[0:5]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``\n",
      "Off\n",
      "there\n",
      "to\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "# And now testing out word tokenizing:\n",
    "mdg_words = nltk.tokenize.word_tokenize(mdg_string)\n",
    "\n",
    "# With the usual check:\n",
    "for word in mdg_words[0:5]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that we are not getting just words, we are getting tokens, which is why the double back ticks are showing up in our list. The NLTK's `word_tokenize()` function may or may not align with what you think should be included: that is why it is important to be able to tokenize as you think is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the basics of tokenization out of the way, let's take a look at how we might count words from within the NLTK. \n",
    "\n",
    "To do this, we are going to use some built-in functionality, creating a particular kind of object and then populating it with a `for` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2064"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist()\n",
    "for sentence in nltk.tokenize.sent_tokenize(mdg_string):\n",
    "    for word in nltk.tokenize.word_tokenize(sentence):\n",
    "        fdist[word] += 1\n",
    "\n",
    "# Let's see what that count is:\n",
    "len(fdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That number is very close to the 1974 we got when we created our own dictionary of words. Let's take a look at what's in there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 640),\n",
       " (',', 556),\n",
       " ('the', 439),\n",
       " ('a', 246),\n",
       " ('``', 223),\n",
       " (\"''\", 210),\n",
       " ('I', 178),\n",
       " ('he', 173),\n",
       " ('of', 171),\n",
       " ('and', 155)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create this looping only over the words\n",
    "# to show that it is not necessary to loop over sentences\n",
    "# But looping over words in sentences can be useful elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 640),\n",
       " (',', 556),\n",
       " ('the', 439),\n",
       " ('a', 246),\n",
       " ('``', 223),\n",
       " (\"''\", 210),\n",
       " ('I', 178),\n",
       " ('he', 173),\n",
       " ('of', 171),\n",
       " ('and', 155)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "```python\n",
    "mdg_dist = nltk.FreqDist()\n",
    "for word in mdg_words:\n",
    "    mdg_dist[word] +=1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you ask Python what kind of data structure freq_dist is,\n",
    "# you'll get a rather unhelpful response, but LOOK ABOVE. What do you see?\n",
    "type(mdg_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can work with freq_dist like any list of tuples\n",
    "\n",
    "for word, frequency in mdg_dist.most_common(10):\n",
    "    print(f\"{word}:  {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_dist comes with a lot of functionality\n",
    "# (See Table 3.1 in Chapter 1 of the NLTK book for more ideas.)\n",
    "\n",
    "mdg_dist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_text = nltk.Text(mdg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_text.concordance(\"eyes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_text.similar(\"eyes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_text.dispersion_plot([\"cossack\", \"knife\", \"bed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# More Texts, More Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_div (text):\n",
    "    lexdiv =len(set(text)) / len(text)\n",
    "    return lexdiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how you can run a function inside an f-string.\n",
    "# Also note the use of backslashes to escape the quotation marks.\n",
    "print(f\"The lexical diversity of \\\"The Most Dangerous Game\\\" is {lex_div(mdg_words):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The lexical diversity of \\\"Heart of Darkness\\\" is {lex_div(hod_words):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a more robust function to handle a series of files\n",
    "def lex_div(a_file):\n",
    "    # open and read the file\n",
    "    the_string =  open(a_file, 'r').read()\n",
    "    # create a list of words\n",
    "    the_words = re.sub(\"[^a-zA-Z']\",\" \", the_string).lower().split()\n",
    "    # divide the vocabular by the total number of words\n",
    "    lexdiv = len (set (the_words)) / len (the_words)\n",
    "    # return this percentage\n",
    "    return lexdiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "\n",
    "for i in data:\n",
    "    the_file = \"../data/1924/texts/\"+i+\".txt\"\n",
    "    lexdiv = lex_div(the_file)\n",
    "    print(f\"{i}: {lexdiv:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hmmm* ... that's quite a range. Referring to the lexical diversities for \"The Most Dangerous Game\" and _Heart of Darkness_, what do you think is at work there? \n",
    "\n",
    "What happens if we add a text as a data point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = \"../data/hamlet.txt\"\n",
    "lex_div(hamlet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Your turn:</b> Write code that explores the possible dimension in play here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leftover Code\n",
    "\n",
    "```python\n",
    "# Create a list of substrings, aka words\n",
    "mdg_words = nltk.tokenize.word_tokenize(mdg_string.lower())\n",
    "\n",
    "# Repeat for \"Heart of Darkness\"\n",
    "hod_string = open('../data/hod.txt', 'r').read()\n",
    "hod_words = nltk.tokenize.word_tokenize(hod_string.lower())\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
