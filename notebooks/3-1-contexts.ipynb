{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-1 Contexts & Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back in notebook 1-2, [Texts and NLTK](https://github.com/johnlaudun/text-as-data/blob/main/notebooks/1-2-texts-and-NLTK-lab.ipynb), we explored some of the ways words accrue meaning through the context of their usage. We return to that now to understand how extracting grammar-based features, keyphrases, entities, n-grams (and collocations) can help us analyze texts more thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is a big library. I'm okay loading it in its entirety when I first start work, but as I work I like to narrow what functionalities I need and slowly change my imports to just what I need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from pathlib import Path\n",
    "import re\n",
    "import nltk # See note above.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "# MPL block\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This custom function actually comes into play later in the notebook, but as a habit I tend to migrate custom functions up in a notebook. Usually I put them just below the imported libraries as part of the overall \"load-in.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The doc string still needs work\n",
    "def processed(a_string):\n",
    "    \"\"\"\n",
    "    processed takes a string and returns a string of lemmas\n",
    "    Requires the following imports:\n",
    "    -------------------------------\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \"\"\"\n",
    "    # Get rid of HTML (or HTML-like) tags\n",
    "    clean = re.sub('<.*?>', '', a_string)\n",
    "    # first we lower-case everything\n",
    "    lowered = clean.lower()\n",
    "    # then tokenize\n",
    "    words = word_tokenize(lowered)\n",
    "    # remove stopwords\n",
    "    # words = [token for token in tokens if token not in stoplist]\n",
    "    # lemmatize\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # rejoin the list of lemmas into a string and return\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def lemmify(a_string):\n",
    "    \"\"\"\n",
    "    processed takes a string and returns a list of lemmas\n",
    "    Requires the following imports:\n",
    "    -------------------------------\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \"\"\"\n",
    "    # Get rid of HTML (or HTML-like) tags\n",
    "    clean = re.sub('<.*?>', '', a_string)\n",
    "    # first we lower-case everything\n",
    "    lowered = clean.lower()\n",
    "    # then tokenize\n",
    "    tokens = word_tokenize(lowered)\n",
    "    # remove stopwords\n",
    "    words = [token for token in tokens if token not in stoplist]\n",
    "    # lemmatize\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Return a list of lemmas\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA\n",
    "# Previously we did this three times.\n",
    "# Now we're just going to use a list to feed an f-string\n",
    "# https://realpython.com/python-f-strings/\n",
    "folders = [\"histories\", \"tragedies\", \"comedies\"]\n",
    "plays = []\n",
    "for folder in folders:\n",
    "    for p in Path(f\"../data/shakespeare/{folder}/\").glob('*.txt'):\n",
    "        with open(p, mode=\"r\", encoding=\"utf-16\") as f:\n",
    "            contents = f.read()\n",
    "            plays.append(contents)\n",
    "\n",
    "# If we have 37, we got them all\n",
    "len(plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data into NLTK Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To explore the plays:\n",
    "# we are going to combine them into one string\n",
    "text = \" \".join(plays)\n",
    "# Break them into a list of tokens\n",
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "# Create an NLTK from this list\n",
    "shakespeare = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 2074 matches:\n",
      "< Shakespeare -- THE FIRST PART OF KING HENRY VI > < from Online Library of \n",
      " < Dead March . Enter the Funeral of King Henry the Fifth ... > < ... attended\n",
      "have consented unto Henry 's death ! King Henry the Fifth , too famous to live\n",
      " to live long ! England ne'er lost a king of so much worth . < /BEDFORD > < GL\n",
      "CESTER > < 1 % > England ne'er had a king until his time . Virtue he had , des\n",
      "ER > < WINCHESTER > < 1 % > He was a king bless 'd of the King of kings . Unto\n",
      " 1 % > He was a king bless 'd of the King of kings . Unto the French the dread\n",
      "ort : The Dauphin Charles is crowned king in Rheims ; The Bastard of Orleans w\n",
      "EXETER > < 4 % > The Dauphin crowned king ! all fly to him ! O ! whither shall\n",
      "ur laments , Wherewith you now bedew King Henry 's hearse , I must inform you \n",
      "And then I will proclaim young Henry king . < /GLOUCESTER > < STAGE DIR > < Ex\n",
      "> To Eltham will I , where the young king is , Being ordain 'd his special gov\n",
      "will not be Jack-out-of-office . The king from Eltham I intend to steal , And \n",
      " Thou art no friend to God or to the king : Open the gates , or I 'll shut the\n",
      "roditor , And not protector , of the king or realm . < /WINCHESTER > < GLOUCES\n",
      " Beaufort , that regards nor God nor King , Hath here distrain 'd the Tower to\n",
      " out of the Tower , To crown himself king and suppress the prince . < /WINCHES\n",
      "s day , against God 's peace and the king 's , we charge and command you , in \n",
      "ce , Third son to the third Edward , King of England . Spring crestless yeomen\n",
      "e , For treason executed in our late king 's days ? And , by his treason stand\n",
      "nry the Fourth , grandfather to this king , Depos 'd his nephew Richard , Edwa\n",
      "tten , and the lawful heir Of Edward king , the third of that descent : During\n",
      "e lords to this Was , for that—young King Richard thus remov 'd , Leaving no h\n",
      " Duke of Clarence , the third son To King Edward the Third ; whereas he From J\n",
      ". > < STAGE DIR > < Flourish . Enter King Henry , Exeter , Gloucester , Warwic\n"
     ]
    }
   ],
   "source": [
    "shakespeare.concordance(\"king\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are dealing with longer texts in your corpus, you might want to think about visualizing where certain words fall within the text. It would be interesting, for example, to see how \"king\" is distributed across the various plays: we could do that with NLTK's `fdist` and a for-loop. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Action:</b> Look up how to create distribution or dispersion plots in Python. This is a common enough need that there are built-in functions in Plotly and Seaborn.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir and lord duke man world prince queen day time crown love word\n",
      "heart one way matter that lady but\n"
     ]
    }
   ],
   "source": [
    "shakespeare.similar(\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time fool king day lady gentleman and love maid heart good word\n",
      "life night is lord place dir thing\n"
     ]
    }
   ],
   "source": [
    "shakespeare.similar(\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building ngram index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banishment ! 't is a lion That I have laboured for the house of York\n",
      "Shall be brought unto the gentleman , and let them hear what pitiful\n",
      "cries they made with this decree ; She 's the matter was , And revel\n",
      "it with groans ; but I was sometime Milan.—Quickly , spirit ! > <\n",
      "SANDS > < /STAGE DIR > < BEATRICE > < 77 % > How 's this ? still-vex\n",
      "'d Bermoothes ; there art thou there , Enforce him with the bitterness\n",
      "of soul To the Fool. < /FRENCH KING > < 86 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"banishment ! 't is a lion That I have laboured for the house of York\\nShall be brought unto the gentleman , and let them hear what pitiful\\ncries they made with this decree ; She 's the matter was , And revel\\nit with groans ; but I was sometime Milan.—Quickly , spirit ! > <\\nSANDS > < /STAGE DIR > < BEATRICE > < 77 % > How 's this ? still-vex\\n'd Bermoothes ; there art thou there , Enforce him with the bitterness\\nof soul To the Fool. < /FRENCH KING > < 86 %\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ngram index...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/STAGE DIR; STAGE DIR; /MARK ANTONY; MARK ANTONY; thou art; thou hast;\n",
      "/DON PEDRO; Sir John; thou shalt; /ANTIPHOLUS SYR.; ANTIPHOLUS SYR.;\n",
      "/DROMIO SYR.; DROMIO SYR.; Thou art; King Henry; dost thou; thou wilt;\n",
      "Good morrow; art thou; /ANTIPHOLUS EPH.\n"
     ]
    }
   ],
   "source": [
    "shakespeare.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the bigram functionality in the NLTK reveals two things: we need to be better about cleaning our texts, and it produces a long list that we need to be tallied. (That is, we don't want to see *all* the bigrams, only the most interesting ones, which means both cleaning, again, and counting and sorting.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SCENE', '1')\n",
      "('1', '>')\n",
      "('>', '<')\n",
      "('<', 'Westminster')\n",
      "('Westminster', 'Abbey.')\n",
      "('Abbey.', '>')\n",
      "('>', '<')\n",
      "('<', 'STAGE')\n",
      "('STAGE', 'DIR')\n",
      "('DIR', '>')\n",
      "('>', '<')\n",
      "('<', 'Dead')\n",
      "('Dead', 'March')\n",
      "('March', '.')\n",
      "('.', 'Enter')\n",
      "('Enter', 'the')\n",
      "('the', 'Funeral')\n",
      "('Funeral', 'of')\n",
      "('of', 'King')\n"
     ]
    }
   ],
   "source": [
    "bigrams = nltk.bigrams(tokens[100:120])\n",
    "\n",
    "for bigram in bigrams:\n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning can mean simply filtering stop tokens, because we want to include those angle brackets, as well as normalizing and possibly (possibly) lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(956676, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Say hello to our old friend:\n",
    "vec = CountVectorizer(preprocessor = processed, ngram_range=(2,3))\n",
    "\n",
    "# matrix of ngrams\n",
    "ngrams = vec.fit_transform(plays)\n",
    "\n",
    "# count frequency of ngrams\n",
    "count_values = ngrams.toarray().sum(axis=0)\n",
    "\n",
    "# list of ngrams\n",
    "vocab = vec.vocabulary_\n",
    "df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()],\n",
    "                            reverse=True)).rename(columns={0: 'frequency', 1:'bigram/trigram'})\n",
    "df_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>bigram/trigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>552</td>\n",
       "      <td>all the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>548</td>\n",
       "      <td>with the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>524</td>\n",
       "      <td>thou art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>521</td>\n",
       "      <td>no more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>520</td>\n",
       "      <td>will not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>519</td>\n",
       "      <td>let me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>503</td>\n",
       "      <td>of this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>502</td>\n",
       "      <td>this is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>497</td>\n",
       "      <td>of your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>493</td>\n",
       "      <td>in my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>491</td>\n",
       "      <td>me to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>482</td>\n",
       "      <td>to my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>479</td>\n",
       "      <td>do not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>469</td>\n",
       "      <td>to me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>465</td>\n",
       "      <td>out of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>462</td>\n",
       "      <td>in his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>461</td>\n",
       "      <td>the world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>437</td>\n",
       "      <td>you shall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>437</td>\n",
       "      <td>there is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>437</td>\n",
       "      <td>that you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    frequency bigram/trigram\n",
       "20        552        all the\n",
       "21        548       with the\n",
       "22        524       thou art\n",
       "23        521        no more\n",
       "24        520       will not\n",
       "25        519         let me\n",
       "26        503        of this\n",
       "27        502        this is\n",
       "28        497        of your\n",
       "29        493          in my\n",
       "30        491          me to\n",
       "31        482          to my\n",
       "32        479         do not\n",
       "33        469          to me\n",
       "34        465         out of\n",
       "35        462         in his\n",
       "36        461      the world\n",
       "37        437      you shall\n",
       "38        437       there is\n",
       "39        437       that you"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ngram[20:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "*Also known as POS tagging.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m redshirts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mFrom the top of the large boulder he sat on, Ensign Tom Davis looked across the expanse of the cave toward Captain Lucius Abernathy, Science Officer Qeeng and Chief Engineer Paul West perched on a second, larger boulder, and thought, Well, this sucks. “Borgovian Land Worms!” Captain Abernathy said, and smacked his boulder with an open palm. “I should have known.” You should have known? How the hell could you not have known? thought Ensign Davis, and looked at the vast dirt floor of the cave, its powdery surface moving here and there with the shadowy humps that marked the movement of the massive, carnivorous worms.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m redshirts \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m(redshirts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "redshirts = \"\"\"\n",
    "From the top of the large boulder he sat on, Ensign Tom Davis looked across the expanse of the cave toward Captain Lucius Abernathy, Science Officer Qeeng and Chief Engineer Paul West perched on a second, larger boulder, and thought, Well, this sucks. “Borgovian Land Worms!” Captain Abernathy said, and smacked his boulder with an open palm. “I should have known.” You should have known? How the hell could you not have known? thought Ensign Davis, and looked at the vast dirt floor of the cave, its powdery surface moving here and there with the shadowy humps that marked the movement of the massive, carnivorous worms.\n",
    "\"\"\"\n",
    "\n",
    "redshirts = word_tokenize(redshirts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('From', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('top', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('large', 'JJ'),\n",
       " ('boulder', 'NN'),\n",
       " ('he', 'PRP'),\n",
       " ('sat', 'VBD'),\n",
       " ('on', 'IN'),\n",
       " (',', ','),\n",
       " ('Ensign', 'NNP'),\n",
       " ('Tom', 'NNP'),\n",
       " ('Davis', 'NNP'),\n",
       " ('looked', 'VBD'),\n",
       " ('across', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('expanse', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('cave', 'NN'),\n",
       " ('toward', 'IN'),\n",
       " ('Captain', 'NNP'),\n",
       " ('Lucius', 'NNP'),\n",
       " ('Abernathy', 'NNP'),\n",
       " (',', ','),\n",
       " ('Science', 'NNP'),\n",
       " ('Officer', 'NNP'),\n",
       " ('Qeeng', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Chief', 'NNP'),\n",
       " ('Engineer', 'NNP'),\n",
       " ('Paul', 'NNP'),\n",
       " ('West', 'NNP'),\n",
       " ('perched', 'VBD'),\n",
       " ('on', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('second', 'JJ'),\n",
       " (',', ','),\n",
       " ('larger', 'JJR'),\n",
       " ('boulder', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('thought', 'VBD'),\n",
       " (',', ','),\n",
       " ('Well', 'NNP'),\n",
       " (',', ','),\n",
       " ('this', 'DT'),\n",
       " ('sucks', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redshirts_tags = nltk.pos_tag(redshirts)\n",
    "redshirts_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short list of the codes above:\n",
    "\n",
    "|Tag | Part of Speech |\n",
    "|----|----------------|\n",
    "|JJ\t | Adjectives     | \n",
    "|NN\t | Nouns          |\n",
    "|RB\t | Adverbs        |\n",
    "|PRP | Pronouns       |\n",
    "|VB\t | Verbs          |\n",
    "\n",
    "But this [alphabetical list](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) on a UPenn web page is really the most helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT to run: Be aware it's long.\n",
    "# For a fuller list:\n",
    "# nltk.help.upenn_tagset()\n",
    "\n",
    "# You have to make sure you download it first:\n",
    "# nltk.download(\"tagsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chunk \"grammar\"\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# Instantiate the chunk parser\n",
    "chunk_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sentence = \"From the top of the large boulder he sat on, Ensign Tom Davis looked across the expanse of the cave toward Captain Lucius Abernathy and thought, Well, this sucks.\"\n",
    "\n",
    "\n",
    "\n",
    "tree = chunk_parser.parse(redshirts_tags)\n",
    "tree.draw()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "370",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
