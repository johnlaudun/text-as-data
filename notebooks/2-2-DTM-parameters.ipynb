{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee921a6-e7b8-4d72-b790-ea3920c3dcab",
   "metadata": {},
   "source": [
    "# Working with a Corpus of Screenplays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21a498-a80a-4ce9-8e00-8d70e1d5b1a1",
   "metadata": {},
   "source": [
    "This notebook is to establish a different route for those working with a smaller number of larger documents. 37 plays is a very small number, but it's a toy corpus to which we all have access. The method should scale to several hundred texts quite easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565bc1d",
   "metadata": {},
   "source": [
    "## Getting the Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f8576c-e0c0-4d2d-bba1-1e5aec803f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03319b49-8f07-4034-a6d3-1867931e6a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA\n",
    "# Change the variable/object below to reflect your genre\n",
    "mysteries = []\n",
    "\n",
    "# Now we populate the object with strings\n",
    "for p in Path(f\"../queue/Mystery\").glob('*.txt'):\n",
    "    with open(p, mode=\"r\") as f:\n",
    "        contents = f.read()\n",
    "        mysteries.append(contents)\n",
    "\n",
    "# If we have the same number as we can see in the folder, we got them all\n",
    "len(mysteries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768abd3",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d845f910",
   "metadata": {},
   "source": [
    "There are two paths forward here. The first time around we are going to keep it simple and simply lowercase and tokenize our words as we have in previous sessions. There is a second path which drops out common words as well as lemmatizing the word types so that there are fewer tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd88339",
   "metadata": {},
   "source": [
    "### Standard Tokenization\n",
    "\n",
    "The SciKit-Learn module comes with a tokenizer built in. It makes a lot of assumptions, but it's not a bad place to start any exploration. Run the code below to see what your vocabulary is, and then try adding the following parameters:\n",
    "\n",
    "- `min_df=`: Start with 2 and iterate up to 5. (Bonus points if you write this as a loop.)\n",
    "- `max_df=`: Conventionally this tends to be by percentage. You can start with 0.99 and work down to 0.8, using 0.05 steps to see the affect.\n",
    "\n",
    "Make sure you understand the **df** that you are minning and maxing.\n",
    "\n",
    "After you have tried both and arrived at a possible optimum, try combining them. The convention for some of these functions which can go long is to use separate lines for each parameter. Jupyter will normally handle the indent for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9805f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 45272)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Basic vectorizer: no tweaks to parameters\n",
    "vectorizer = CountVectorizer(  )\n",
    "\n",
    "# fit the model to the data \n",
    "matrix = vectorizer.fit_transform(mysteries)\n",
    "\n",
    "# We'll need these later\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# This will repeat our screenplay count\n",
    "# but also report our overall vocabulary FOR ALL SCREENPLAYS\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d15a19",
   "metadata": {},
   "source": [
    "Rows = \n",
    "\n",
    "min_df = 1: 45272\n",
    "min_df = 2: 24344\n",
    "min_df = 3: 18553"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cefa68-e072-4856-82fe-86ccdfeaf463",
   "metadata": {},
   "source": [
    "### Custom Tokenization\n",
    "\n",
    "Next we're going to build a custom text \"pre-processor\" that we can use with a variety of Sci-Kit Learn (and perhaps other) functions. By building our own, in effect, tokenizer, we make it possible to be more in control of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03ea54-9a08-42e5-8aeb-e226084aefa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def processit(a_string):\n",
    "    \"\"\" Requires nltk \"\"\"\n",
    "    # first we lower-case everything\n",
    "    lowered = a_string.lower()\n",
    "    # then tokenize\n",
    "    tokens = word_tokenize(lowered)\n",
    "    # remove stopwords\n",
    "    words = [token for token in tokens if token not in stop_words]\n",
    "    # lemmatize\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # rejoin the list of lemmas into a string and return\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c2235e-ae33-482f-a916-3af460a9b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic vectorizer with our preprocessor added\n",
    "vectorizer = CountVectorizer(min_df = 2, preprocessor = processit)\n",
    "\n",
    "# fit the model to the data \n",
    "matrix_pp = vectorizer.fit_transform(mysteries)\n",
    "\n",
    "# We'll need these later\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# without our preprocessor\n",
    "# there are 11491 features\n",
    "matrix_pp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594ef8d",
   "metadata": {},
   "source": [
    "## Creating and Saving a Dataframe\n",
    "\n",
    "Now that we have our corpus transformed into a document-term matrix (DTM), it would be nice if we could not only make it more computable but also to make it storable as a file. That way, we need not recreate the DTM every time we want to work with it: we can simply load it from a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54150b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000x</th>\n",
       "      <th>001</th>\n",
       "      <th>003</th>\n",
       "      <th>008</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>0134</th>\n",
       "      <th>014</th>\n",
       "      <th>...</th>\n",
       "      <th>zut</th>\n",
       "      <th>zweimal</th>\n",
       "      <th>zx</th>\n",
       "      <th>zy</th>\n",
       "      <th>zying</th>\n",
       "      <th>zzi</th>\n",
       "      <th>zzzapppp</th>\n",
       "      <th>zzzzp</th>\n",
       "      <th>zzzzt</th>\n",
       "      <th>zzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  000x  001  003  008  00pm  01  0134  014  ...  zut  zweimal  zx  \\\n",
       "0   0    0     0    0    0    0     0   0     0    0  ...    0        0   0   \n",
       "1   0    1     0    0    0    0     0   0     0    0  ...    0        0   0   \n",
       "2   2    2     0    0    0    0     0   0     0    0  ...    0        0   0   \n",
       "3   0    0     0    0    0    0     0   0     0    0  ...    0        0   0   \n",
       "4   0    0     0    0    0    0     0   0     0    0  ...    0        0   0   \n",
       "\n",
       "   zy  zying  zzi  zzzapppp  zzzzp  zzzzt  zzzzzzzz  \n",
       "0   0      0    0         0      0      0         0  \n",
       "1   0      0    0         0      0      0         0  \n",
       "2   0      0    0         0      0      0         0  \n",
       "3   0      0    0         0      0      0         0  \n",
       "4   0      0    0         0      0      0         0  \n",
       "\n",
       "[5 rows x 45272 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load up a dataframe with our DTM\n",
    "df = pd.DataFrame(matrix.toarray(), \n",
    "                  columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "# Check our work:\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48872276",
   "metadata": {},
   "source": [
    "It would be nice to have each of those rows labeled. This is less complicated than it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51fc80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['losthighway', 's.darko', 'curiouscaseofbenjaminbuttonthe', 'orphan', 'cherryfalls']\n"
     ]
    }
   ],
   "source": [
    "# First we need to get the file names from the files\n",
    "titles= []\n",
    "\n",
    "for p in Path('../queue/Mystery/').glob('*.txt'):\n",
    "    with open(p, mode=\"r\", encoding=\"utf-16\") as f:\n",
    "        title = p.name[:-4]\n",
    "        titles.append(title)\n",
    "\n",
    "# Check\n",
    "print(titles[50:55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8514258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column in our data frame\n",
    "# and then populate it with our list of titles\n",
    "df[\"title\"] = titles\n",
    "\n",
    "# Set the title to be the first column (the index) of the dataframe\n",
    "df.set_index('title', inplace = True)\n",
    "\n",
    "# See it:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d349dcc",
   "metadata": {},
   "source": [
    "Having done all this work, let's save it to a file. We are saving to a CSV file, but you can also save to an Excel file, if that's more useful to you. (If you do want to try saving to Excel, please also save to a CSV file: it will be easier to load into a dataframe in our next session.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/mysteries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cda645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Size of the CSV\n",
    "csv_size = os.path.getsize('../data/mysteries.csv')\n",
    "\n",
    "# Size of the directory\n",
    "texts_size = sum(d.stat().st_size for d in os.scandir('../queue/Mystery') if d.is_file())\n",
    "\n",
    "# Let's compare the sizes in megabytes (millions of bytes)\n",
    "print(csv_size/10**6, texts_size/10**6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "370",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
