{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYOC: Build Your Own Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot in this notebook. One reason for that is that scraping websites is more art than science: you have to be patient and be prepared either to tune your current method or to change to a different method. After the recap below, I discuss four methods:\n",
    "\n",
    "- CLI tool `wget`\n",
    "- Python `urllib` in conjunction with `BeautifulSoup`\n",
    "- Python `selenium` in conjunction with `BeautifulSoup` *and* `wget`\n",
    "- Python `praw` library to work with Reddit\n",
    "\n",
    "My best advice is to read quickly through this notebook just to get a sense of all the possibilities, then to turn to your own corpus and discover for yourself what works. Be prepared for bumps and bruises along the way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of Reasons to Create Your Own Corpus\n",
    "\n",
    "**The Big Picture (for our small data)**\n",
    "- Any machine learning application is primarily focused on discovering **signal**(s) within noise. \n",
    "- Extraction is done via **feature analysis**, determining which features, properties, dimensions best encode its meaning and underlying structure. \n",
    "- Determining a **representation** requires us to define the units (of language)—the things we count, measure, analyze, and learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What is a corpus?**\n",
    "- A collection of related documents (that contain natural language)\n",
    "- Size matters less but good design matters more. (ChatGPT is proof that large size can overcome some bad design decisions. A variety of focused BERTs reveal that design can beat size.)\n",
    "- Typically, a corpus can be broken down into categories (or genres) of documents, which can emerge as principal components, centroids, neighborhoods, etc.\n",
    "- Corpora can be text (data) only or they can contain metadata: the titles of novels, the date a tweet was published, the name of the user who posted the TikTok, the subreddit in which a post appeared, the number of comments or like any of these have received."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the components?**\n",
    "- Documents can be broken into paragraphs (units of discourse which conventionally express one idea or action), into sentences (units of syntax), into words and punctuation. (Words can be further broken down into syllables and characters, if interested.)\n",
    "- The unit of analysis is the decision of the analyst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of Domain-Specific Corpora**\n",
    "- A corpus that is relatively focused on a specific domain is easier to analyze and model than one made up of mixed domains.\n",
    "- By fitting models in a narrower context, the prediction space is smaller and more specific, and therefore better able to handle the flexible aspects of language. (Again, ChatGPT versus all the BERTs.)\n",
    "- How do we do it: scraping, RSS ingestion, API, finding a raw text corpus already out there. (See: Kaggle, Harvard Dataverse, etc.)\n",
    "- Acquisition is the first step. Determining how to structure and manage the data, which includes “cleaning”,  is the next step that is often overlooked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Places to get stuff:\n",
    "\n",
    "**Books**\n",
    "* Project Gutenberg: (http://gutenberg.org/)\n",
    "* Google Books (http://books.google.com)\n",
    "* EEBO (http://quod.lib.umich.edu/e/eebogroup/)\n",
    "* ECCO (http://quod.lib.umich.edu/e/ecco/)\n",
    "* Evans (http://quod.lib.umich.edu/e/evans/)\n",
    "\n",
    "**Scholarship & Science**\n",
    "* JSTOR DFR (http://dfr.jstor.org/)\n",
    "* Open Access: PMC Open Access Set, PLoS, BioMed Central\n",
    "\n",
    "**Social Media**\n",
    "* The author of _Mining the Social Web_ (O'Reilly) has made the code available (on GitHub of course). Here's his website: [Mining the Social Web](https://miningthesocialweb.com).\n",
    "* [Twitter/X APIs](https://dev.x.com)\n",
    "* [Facebook API](https://developers.facebook.com)\n",
    "* [Reddit API](https://developers.reddit.com/docs/api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Troubles with Access and Quality\n",
    "\n",
    "The elephant in the room is **copyright**. The reason so much work is done with older books is that they are out of copyright. Getting access to contemporary books is difficult and/or expensive and is largely the purview of well-funded institutions. \n",
    "\n",
    "The next issue is quality. Most older materials have been digitized, which means a picture of the page was taken and then run through optical character recognition software (OCR). Some texts are more thoroughly checked than others. (Crowd-sourcing can vary: it all depends on the community.) \n",
    "\n",
    "There was a real rush to digitize things in the 2000s and 2010s, but those efforts were often underfunded and underconsidered. E.g., Louisiana Digital Library is kind of a mess. This doesn't mean you shouldn't use resources from under-resourced agencies and organizations, but it does mean that you should do hand-checking. (Depending on size, you may want to find a way to give your material either a thorough check or a considered random check.) OCR scanning erros can lead ro signals that are weak or just plain wrong.\n",
    "\n",
    "re: Social media, see: Boyd and Crawford 2011 (SSRN: 1926431).\n",
    "\n",
    "Not only is OCR problematic, but automated tasks, like named entity extraction, are also questionable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CLI Tool: `wget`\n",
    "\n",
    "Sometimes CLI tools, like `wget`, are more powerful than GUI tools. The key difference is that GUI tools are easier to use at first, but repetitive tasks are difficult or expensive (in terms of time). CLI tools are a little more difficult at first, but once you have an established collection of them, they are not only easier to use but just plain easier. \n",
    "\n",
    "**`wget`** is one of those tools. E.g.:\n",
    "\n",
    "    % wget -r -l 1 -w 2 --limit-rate=20k https://www.cs.cmu.edu/~spok/grimmtmp/\n",
    "\n",
    "Since CLI commands can often appear like magical incantations, let's break down what's happening in the line above:\n",
    "\n",
    "* `wget` is the program we are going to use. It wants to know where to go and how to proceed.\n",
    "* `-r` (or `--recursive`) turns on recursive retrieving (up to 5 directories deep). \n",
    "* `-l 1` (or`--level=1`) keeps the depth to 1.\n",
    "* `-w 2` gives the amount of time to wait between retrievals. (Two seconds lessens the server load.)\n",
    "* `--limit-rate=20k` sets the retrieval rate to 20kB/s. (This is being polite in a shared connection setting.)\n",
    "\n",
    "K.M. Kinnaird and I used `wget` to \"crawl\" the TED website and download transcripts, talk and speaker descriptions, and comments pages. I have also used it, as the URL in the example above suggests, to grab the texts of Grimms' fairy tales. It's a truly useful tool, and, luckily for us, available through the **conda** package management system. *Yay!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Libraries: `urllib`\n",
    "\n",
    "#### Case 1: Downloading Files from a Directory\n",
    "\n",
    "Some sites do not like being crawled by `wget` and they will return **ERROR 403: Forbidden**. In those cases where you can see the directory contents but it is probably the case that the site has been configured so that directores cannot be browsed directly, you will have to plot an alternative path.\n",
    "\n",
    "Sometimes you can use `wget` to retrieve a list of files and then feed that list to a Python script, or sometimes you can do everything you need from within Python. When I was helping someone working with treaties between Plains Indians and various government agencies, we developed the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# To use this script, the user needs to provide the three values below: \n",
    "# myurl, myfilter, mydirectory\n",
    "# Please make sure `mydirectory` is already created before running\n",
    "\n",
    "myurl = \"http://digital.library.okstate.edu/kappler/Vol2/Toc.htm\"\n",
    "myfilter = \"http://digital.library.okstate.edu/kappler/Vol2/treaties/\"\n",
    "mydirectory = \"/Users/me/Desktop/downloadedfiles/\"\n",
    "\n",
    "myconnection = urllib.request.urlopen(myurl)\n",
    "myhtml = myconnection.read()\n",
    "mysoup = BeautifulSoup(myhtml, \"lxml\")\n",
    "mylinks = mysoup.find_all('a')\n",
    "\n",
    "all_links = []\n",
    "for tag in mylinks:\n",
    "    link = tag.get('href',None)\n",
    "    if link is not None:\n",
    "        all_links.append(link)\n",
    "\n",
    "myresults = [k for k in all_links if myfilter in k]\n",
    "\n",
    "for result in myresults:\n",
    "    remotefile = urllib.request.urlopen(result)\n",
    "    localfile = open(mydirectory+result.replace(myfilter, ''),'wb')\n",
    "    localfile.write(remotefile.read())\n",
    "    localfile.close()\n",
    "    remotefile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Case 2: Working with an Content Management System\n",
    "\n",
    "What happens when the texts with which you want to work are not sitting in a directory, but are in a content management system (CMS)? Helping somone interested in Paul Laurence Dunbar's poetry and fiction, we started with the previous script to map how the CMS generated URLs. Once we had a map, we were pretty sure we had a way to get what we wanted.\n",
    "\n",
    "Here is the link for the digital archive of Dunbar’s work at Wright State: http://www.libraries.wright.edu/special/dunbar/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Screenshot of Dunbar Archive Web Page](../assets/2-1-ScreenShot_Dunbar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we click on the \"poetry\" link in the lefthand navigation pane, and then hover over one of the books (see image above), we see the following URL: \n",
    "\n",
    "    http://www.libraries.wright.edu/special/dunbar/explore?book=8\n",
    "\n",
    "Clicking on a book, takes us to a table of contents, with a series of links like this:\n",
    "\n",
    "    http://www.libraries.wright.edu/special/dunbar/explore?book=9&id=236\n",
    "\n",
    "The `id`s are not sequential within a book; however, by playing with the URLs in a browser, it looks like you can insert an asterisk into portion of the URL that identifies the book, `book=*`, and still get back results on simply the `id=`:\n",
    "\n",
    "    http://www.libraries.wright.edu/special/dunbar/explore?book=*&id=99\n",
    "\n",
    "In fact, after a little experimentation of just typing in numbers and changing the `id` number and getting back results, it looks like we just need to iterate through all the `id`s. If we start with `1`, how far up do we need to go? Since I saw numbers in the 300s earlier, I am going to start with 400 and go up by 100 until I get no results and then narrow by 10s and then 1s until I know where to stop ... and it appears we stop at 433.\n",
    "\n",
    "Now let's go build, er, revise us some code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "baseurl = \"http://www.libraries.wright.edu/special/dunbar/explore?book=*&id=\"\n",
    "mydirectory = \"/Users/jjl/Desktop/downloadedfiles/\"\n",
    "\n",
    "mylist = []\n",
    "for i in range (1, 434):\n",
    "    link = baseurl+str(i)\n",
    "    mylist.append(link)\n",
    "\n",
    "for link in mylist:\n",
    "    remotefile = urllib.request.urlopen(link).read()\n",
    "    soup = BeautifulSoup(remotefile, \"lxml\")\n",
    "    div = soup.find('div', 'bookContain-right')\n",
    "    localfile = open(mydirectory+link.replace(baseurl, '')+\".html\",'wt')\n",
    "    localfile.write(str(div.encode('utf-8')))\n",
    "    localfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code works, and it returns only the contents of the desired `div`:\n",
    "\n",
    "    <div class=\"bookContain-right\">\n",
    "\n",
    "But the contents remain ugly. At the very least, some regex is needed to clean up some of the escaped characters: those that begin with a backslash. Perhaps better would be to use `html2text` to convert the documents to plain text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "baseurl = \"http://www.libraries.wright.edu/special/dunbar/explore?book=*&id=\"\n",
    "mydirectory = \"/Users/jjl/Desktop/downloadedfiles/\"\n",
    "\n",
    "mylist = []\n",
    "for i in range (1, 2):\n",
    "    link = baseurl+str(i)\n",
    "    mylist.append(link)\n",
    "\n",
    "for link in mylist:\n",
    "    remotefile = urllib.request.urlopen(link).read()\n",
    "    soup = BeautifulSoup(remotefile, \"lxml\")\n",
    "    div = soup.find('div', 'bookContain-right')\n",
    "    text = html2text.html2text(str(div))\n",
    "    localfile = open(mydirectory+link.replace(baseurl, '')+\".txt\",'wt')\n",
    "    localfile.write(str(text))\n",
    "    localfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Library: `selenium`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I worked for the Army, I was interested in possibly writing a paper about how the Army's thinking about China had changed over time. My goal was to build a corpus of all the materials available from the Army War College, which has both a journal as well as theses written by students and papers by faculty, West Point, and TRADOC. \n",
    "\n",
    "Trying to keep things as simple as possible, I started by trying to scrape the AWC Press website, which uses BePress, focused on using the `requests` library. (See [DataQuest's tutorial](https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/) as a reminder/guide.) Using a URL I that worked in my browser, I met with failure:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "page = requests.get(\"https://press.armywarcollege.edu/do/search/?q=china&start=25&context=18225338&facet=#query-results\")\n",
    "page\n",
    "```\n",
    "\n",
    "```\n",
    "ConnectionError: HTTPSConnectionPool(host='press.armywarcollege.edu', port=443): Max retries exceeded with url: /do/search/?q=china&start=25&context=18225338&facet= (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fb9318dab90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
    "```\n",
    "\n",
    "After trying a few other approaches, including my old friend `wget`, I turned to  Selenium, with some help from [ScrapFly's \"Web Scraping with Selenium and Python\"](https://scrapfly.io/blog/web-scraping-with-selenium-and-python/). It requires the `chromedriver`--both `selenium` and `chromedriver` are available through **conda**.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "In my experience, the chromedriver wants something fairly specific: <br/><code>export PATH=$PATH:/Users/jl/miniconda3/bin</code>. <br/>\n",
    "Make sure to add it to your PATH — the `.zshrc` file on macOS.</div>\n",
    "\n",
    "Once you connect, you can turn to the trusty parser of web pages, `BeautifulSoup`, and drill down into the search page HTML to get the URLs you need to create a list of URLs. In my case I decided to feed the list to `wget` and let it fetch the PDFs from the AWC Press website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# ==== To run headless:\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# configure webdriver\n",
    "# options = Options()\n",
    "# options.headless = True  # hide GUI\n",
    "# options.add_argument(\"--window-size=1920,1080\")  # set window size to native GUI size\n",
    "# options.add_argument(\"start-maximized\")  # ensure window is full-screen\n",
    "\n",
    "# driver = webdriver.Chrome(options=options)\n",
    "# ====\n",
    "\n",
    "for i in range(0, 475, 25):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(f\"https://press.armywarcollege.edu/do/search/?q=china&start={i}&context=18225338&facet=#query-results\")\n",
    "    time.sleep(15) # Selenium is a bit slow, so patience.\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    soupy_pdfs = soup.find_all('a', class_=\"pdf\")\n",
    "    pdfs = [str(item) for item in soupy_pdfs]\n",
    "    with open(\"pdfurls4.txt\", \"a\") as file:\n",
    "        file.writelines(s + '\\n' for s in pdfs)\n",
    "    driver.close() # I thought the driver remaining open was the pbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AltPath: Using the Search Results File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Use our search results to create URLs\n",
    "with open('../notes/AWC-search_results.txt', 'r') as the_file:\n",
    "    urls = the_file.read().splitlines()\n",
    "\n",
    "# Create an empty list\n",
    "pdflinks = []\n",
    "\n",
    "# And then fill it\n",
    "for url in urls:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    link = soup.find('a', attrs = {'id':'alpha-pdf'})['href']\n",
    "    pdflinks.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdflinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../notes/pdfurls.txt', 'w') as f:\n",
    "    f.writelines(i + '\\n' for i in pdflinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit's `praw`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final example uses the `praw` library to download the 100 hot posts in `r/conspiracy` and save them to a dataframe (and later to a CSV file). \n",
    "\n",
    "*For those interested, it is also possible to use the ID for each post to grab all the comments. You can then add the comments to the dataframe, all of which can then be saved as a CSV.*\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Warning!</b> Please note that most APIs will require you to set up credentials. You may be tempted to include those credentials in your script. <b>NEVER INCLUDE YOUR CREDENTIALS IN YOUR CODE.</b> It is too easy to commit and push and before you know it, your credentials are out on the big, bad web. Instead, save your credentials in a separate file which sits outside the repository. \n",
    "</div>\n",
    "\n",
    "Loading credentials is fairly easy: in the code below I have saved them to my `.zshrc` file and I grab them using the `os.getenv()` function. Easy peasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# SECRET DECODER RING\n",
    "webber_id = os.getenv(\"REDDIT_API_ID\")\n",
    "webber_secret = os.getenv(\"REDDIT_API_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"\",\n",
    "    client_secret=\"\",\n",
    "    user_agent=\"webber\"\n",
    "    )\n",
    "print(reddit.read_only)\n",
    "# try:\n",
    "#     print(\"Authenticated as {}\".format(reddit.user.me()))\n",
    "# except ResponseException:\n",
    "#     print(\"Something went wrong during authentication\")\n",
    "\n",
    "# print(reddit.auth.url(scopes=[\"identity\"], state=\"...\", duration=\"permanent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot_posts = reddit.subreddit('conspiracy').hot(limit=5)\n",
    "# for post in hot_posts:\n",
    "#     print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conspiracy = reddit.subreddit('conspiracy')\n",
    "\n",
    "posts = []\n",
    "for post in conspiracy.hot(limit=100):\n",
    "    posts.append([post.title, \n",
    "                  post.score, \n",
    "                  post.id, \n",
    "                  post.url, \n",
    "                  post.num_comments, \n",
    "                  post.selftext, \n",
    "                  post.created])\n",
    "\n",
    "posts = pd.DataFrame(posts,\n",
    "                     columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created'])\n",
    "\n",
    "posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts.to_csv('c2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Braxton's Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# To use this script, the user needs to provide the three values below: \n",
    "# myurl, myfilter, mydirectory\n",
    "# Please make sure `mydirectory` is already created before running\n",
    "\n",
    "myurl = \"https://www.azlyrics.com/m/mfdoom.html\"\n",
    "\n",
    "\n",
    "myconnection = urllib.request.urlopen(myurl)\n",
    "myhtml = myconnection.read()\n",
    "mysoup = BeautifulSoup(myhtml, \"lxml\")\n",
    "mylinks = mysoup.find_all(class_=\"listalbum-item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mylinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in mylinks[0:4]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydivs = mysoup.find_all(class_=\"listalbum-item\")\n",
    "\n",
    "links = []\n",
    "for div in mydivs[0:2]:\n",
    "    the_link = div.find('a')['href']\n",
    "    links.append(the_link)\n",
    "\n",
    "\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now we have some piece of the URL we want to use in a list.\n",
    "\n",
    "Let's take a look at the URL we are trying to replicate:\n",
    "```\n",
    "https://www.azlyrics.com/lyrics/mfdoom/meddlewithmetal.html\n",
    "```\n",
    "\n",
    "It looks like all we need to do is prepend `https://www.azlyrics.com` to build the full URL. (Note that we have a slash already, so we don't need the trailing slash in our base URL.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Desktop/downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseURL = \"https://www.azlyrics.com\"\n",
    "myfilter = \"https://www.azlyrics.com/lyrics/mfdoom/\"\n",
    "\n",
    "# mydirectory = \"/Users/me/Desktop/downloaded/\"\n",
    "\n",
    "for link in links:\n",
    "    remotefile = urllib.request.urlopen(baseURL+link)\n",
    "    filename = link.replace(myfilter, '')\n",
    "    print(filename)\n",
    "    # localfile = open(link.replace(myfilter, ''),'wb')\n",
    "    # localfile.write(remotefile.read())\n",
    "    # localfile.close()\n",
    "    # remotefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseURL = \"https://www.azlyrics.com\"\n",
    "myfilter = \"/lyrics/mfdoom/\"\n",
    "\n",
    "# mydirectory = \"/Users/me/Desktop/downloaded/\"\n",
    "\n",
    "for link in links:\n",
    "    remotefile = urllib.request.urlopen(baseURL+link)\n",
    "    filename = link.replace(myfilter, '')\n",
    "    localfile = open(filename,'wb')\n",
    "    localfile.write(remotefile.read())\n",
    "    localfile.close()\n",
    "    remotefile.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "370",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
