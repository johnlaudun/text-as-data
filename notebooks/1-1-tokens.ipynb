{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# From Files to Strings to \"Texts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First we open the file:\n",
    "opened_file = open('../data/mdg.txt', 'r')\n",
    "\n",
    "# Then we read the file:\n",
    "text_as_read = opened_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='../data/mdg.txt' mode='r' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "# What happens when we try to print the opened file?\n",
    "print(opened_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The work of the two lines above can also be achieved in one line.\n",
    "mdg = open('../data/mdg.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mdg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44236"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mdg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Off there to the right -- somewhere -- is a large island,\" said Whitney. \"It's rather a mystery -- \n"
     ]
    }
   ],
   "source": [
    "print(mdg[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Nellie,', 'a', 'cruising', 'yawl,', 'swung', 'to', 'her', 'anchor', 'without', 'a', 'flutter', 'of', 'the', 'sails,', 'and', 'was', 'at', 'rest.', 'The', 'flood', 'had', 'made,', 'the', 'wind', 'was', 'nearly', 'calm,', 'and', 'being', 'bound', 'down', 'the', 'river,', 'the', 'only', 'thing', 'for', 'it', 'was', 'to', 'come', 'to', 'and', 'wait', 'for', 'the', 'turn', 'of', 'the', 'tide.']\n"
     ]
    }
   ],
   "source": [
    "# One way to include a block of text is to use triple quotes.\n",
    "# It doesn't matter if the quotes are double or single.\n",
    "text = \"\"\"\n",
    "The Nellie, a cruising yawl, swung to her anchor without a flutter of\n",
    "the sails, and was at rest. The flood had made, the wind was nearly\n",
    "calm, and being bound down the river, the only thing for it was to come\n",
    "to and wait for the turn of the tide.\n",
    "\"\"\"\n",
    "\n",
    "split_text = text.split()\n",
    "\n",
    "print(split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nThe Nellie', ' a cruising yawl', ' swung to her anchor without a flutter of\\nthe sails', ' and was at rest. The flood had made', ' the wind was nearly\\ncalm', ' and being bound down the river', ' the only thing for it was to come\\nto and wait for the turn of the tide.\\n']\n"
     ]
    }
   ],
   "source": [
    "comma_text = text.split(\",\")\n",
    "print(comma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Nellie,', 'a', 'cruising', 'yawl,', 'swung', 'to', 'her', 'anchor', 'without', 'a', 'flutter', 'of', 'the', 'sails,', 'and', 'was', 'at', 'rest.']\n"
     ]
    }
   ],
   "source": [
    "# Whenever you have extra long text in a code example that breaks across lines,\n",
    "# you enclose it within three quotation marks to let Python know that. \n",
    "# (This is one reason to load text from a file.)\n",
    "\n",
    "sentence = \"\"\"The Nellie, a cruising yawl, swung to her anchor without a flutter of the sails, and was at rest.\"\"\"\n",
    "\n",
    "split = sentence.split()\n",
    "print(split)\n",
    "# print(\"This set has {} items:\".format(len(set(split))), set(split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'The' == 'the'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This set has 17 items: {'was', 'to', 'rest.', 'cruising', 'at', 'nellie,', 'a', 'flutter', 'her', 'sails,', 'anchor', 'and', 'without', 'the', 'of', 'yawl,', 'swung'}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"The Nellie, a cruising yawl, swung to her anchor without a flutter of the sails, and was at rest.\"\"\"\n",
    "\n",
    "split = sentence.lower().split()\n",
    "\n",
    "print(\"This set has {} items:\".format(len(set(split))), set(split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Regex to Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the regular expression library\n",
    "import re\n",
    "\n",
    "mdg_words = re.sub(\"[^a-zA-Z']\",\" \", mdg).lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick walkthrough might help a little:\n",
    "\n",
    "* **`import re`** tells the script to import the regular expression module which comes bundled with every Python installation but doesn't get loaded into our workspace unless we tell it to do so.\n",
    "* **`mdg_words`** is the object we are creating: everything to the right is the process by which we create the object.\n",
    "* We aren't going to discuss the regular expression substitution, **`re.sub()`** that gets done here, except to note that you should read the stuff inside the parentheses like this: `(find this pattern, substitute this, in this text)` -- in this case I am telling it to find things that are **not** (`[^ ]` is called a *negated set*) letters (big and small) or apostrophes and replace them with spaces. \n",
    "* **`.lower()`** is a method you can apply to strings that makes everything lower case -- otherwise \"The\" and \"the\" are two different keys. \n",
    "* **`.split()`** is the string method discussed above and we're using its default setting of splitting on white spaces, of which we have plenty, since we have replaced everything except for letters and apostrophes with white space.\n",
    "\n",
    "The **`split()`** method turns our string into a **list**. In this case, a list of words that are in the same order as they are in the original text. (The computer has no reason to disturb this order.)\n",
    "\n",
    "If we ask how long the list is, we should come back with a reasonably close count of the words -- don't forget we kept apostrophes, and while most are buried inside contractions, there may be some loose apostrophes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in text: 8017.\n"
     ]
    }
   ],
   "source": [
    "print('Words in text: {}.'.format(len(mdg_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['off', 'there', 'to', 'the', 'right', 'somewhere', 'is', 'a', 'large', 'island', 'said', 'whitney', \"it's\", 'rather', 'a', 'mystery', 'what', 'island', 'is', 'it', 'rainsford', 'asked', 'the', 'old', 'charts', 'call', 'it', \"'ship\", 'trap', 'island', \"'\", 'whitney', 'replied', 'a', 'suggestive', 'name', \"isn't\", 'it', 'sailors', 'have', 'a', 'curious', 'dread', 'of', 'the', 'place', 'i', \"don't\", 'know', 'why']\n"
     ]
    }
   ],
   "source": [
    "# We can slice strings and lists, \n",
    "# but lists of words are more human-like than strings of characters.\n",
    "# Weird, but true.\n",
    "\n",
    "print(mdg_words[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK Tokenizers <a name=\"nltktoken\"></a>\n",
    "\n",
    "Whether you develop your own tokenizer somewhere along the way or not is entirely up to you and the nature of the projects which you undertake. You may never find yourself doing so. There are certainly a lot of already available options, a number of which are packaged with the Natural Language Toolkit, more often called by its acronym \"NLTK\", which, by the way, is the same way we call it in Python: \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "```\n",
    "\n",
    "If we were to run the line above, we would probably wait a few seconds as the NLTK library loaded -- it's quite large. Because it is a large library, and we really don't need all its functionality, most people don't load all of it -- and isn't it cool that we can load only the parts we need! This is handy as you begin to work with larger scripts and larger data sets: keeping your workspace as tidy, and as small, becomes a necessity. And, in some cases, it's actually easier to use certain tools singly and with a particular name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK's WhitespaceTokenizer <a name=\"whitespace\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this instance, we are going to tell Python that we only want \n",
    "# one particular tool from the larger toolkit:\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_tokens = WhitespaceTokenizer().tokenize(mdg.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Please note that we now have two versions of \"The Most Dangerous Game\" rendered as a list in Python: **`mdg_words`** and **`mdg_tokens`**. We created the former using regex and the latter with the NLTK WhitespaceTokenizer. We could have re-used the former name, and that would have, in effect, replaced the older list with the newer one. Please keep this in mind as you work, and also remember to use sensible names for the objects you create. (The more self-explanatory things are, the better as your code grows.)\n",
    "\n",
    "Now, let's take a look at what kind of object this is, how big it is, and then let's print the first 50 items -- I use 50 as a convenient, quick look, but you can adjust the number down to 25 or up to 100 or some other number that you find more useful. I often change the numbers several times, contracting and expanding as I feel the need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"off', 'there', 'to', 'the', 'right', '--', 'somewhere', '--', 'is', 'a', 'large', 'island,\"', 'said', 'whitney.', '\"it\\'s', 'rather', 'a', 'mystery', '--', '\"', '\"what', 'island', 'is', 'it?\"', 'rainsford', 'asked.', '\"the', 'old', 'charts', 'call', 'it', \"'ship-trap\", 'island,\\'\"', 'whitney', 'replied.', '\"a', 'suggestive', 'name,', \"isn't\", 'it?', 'sailors', 'have', 'a', 'curious', 'dread', 'of', 'the', 'place.', 'i', \"don't\"]\n"
     ]
    }
   ],
   "source": [
    "print(mdg_tokens[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK's Word_Tokenize <a name=\"word\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be clear, splitting contractions is actually a feature from the point of view of natural language processing. To follow that form of processing, you can use NLTK's **`word_tokenize()`**. As always, you can import the entire `nltk` and then simply use it in your code by writing `tokens = tokenize.word_tokenize(your_text)` or by importing it specifically as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'off', 'there', 'to', 'the', 'right', '--', 'somewhere', '--', 'is', 'a', 'large', 'island', ',', \"''\", 'said', 'whitney', '.', '``', 'it', \"'s\", 'rather', 'a', 'mystery', '--']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(mdg.lower())\n",
    "\n",
    "print(tokens[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the contraction *it's* has been split into *it* and *'s*. \n",
    "\n",
    "Finally, we should note that the preferred practice when working with the Penn Treebank word_tokenizer is to use it in combination with the sentence tokenizer. What's that you say? There's a function in the NLTK that will break my text into sentences? Yes, there is, and in a subsequent notebook, working again with \"The Most Dangerous Game,\" we will explore its utility. For now, let's just make sure we know how to use it, and, anticipating some control structures, how to use it in combination with the word tokenizer.\n",
    "\n",
    "First, let's import the sentence tokenizer, create a list of sentences, and have a look at the first five sentences it creates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(mdg.lower())\n",
    "\n",
    "print(sentences[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a little hard to read, what if we write a `for` loop to make the printout a bit more readable? There's three ways to do add a line break after a bit of text in Python, one is to let the algorithm do itself. This often works: `print(whatever)` The other is a weird bit of code-lore that always works: add a comma after the item you are printing -- `print(whatever,)`. Another way is to add a newline character to whatever you are printing: `print(whatever + \"\\n\")`. I'm lazy, so I used the mysterious comma technique below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sentence in sentences[0:5]:\n",
    "    print(sentence,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from even this limited example, the sentence tokenizer is not perfect: note how  it splits the sentence with a quoted question into two. Yikes. If things like this matter to you, you are going to need to write your own tokenizer -- which is not as hard as it sounds right now to you, but it is additional work when maybe you want to be examining your text.\n",
    "\n",
    "Now, let's see if we can't combine the two tokenizers so that we get the preferred context for the word tokenizer, which is a single sentence, and yet all of our text is tokenized...\n",
    "\n",
    "If we first try to pass the output of one, the sentence tokenizer, as input to the other, the word tokenizer, we will get a `TypeError`, revealing that because both tokenizers expect strings as input and we are feeding the output of the sentence tokenizer, which is a list, into the word tokenizer, something's gonna break:\n",
    "\n",
    "```python\n",
    "sentences = sent_tokenize(mdg)\n",
    "tokens = word_tokenize(sentences)\n",
    "print(tokens[0:20])\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-34-b0aa2f5e281c> in <module>\n",
    "      1 sentences = sent_tokenize(mdg)\n",
    "----> 2 tokens = word_tokenize(sentences)\n",
    "      3 print(tokens[0:20])\n",
    "```\n",
    "\n",
    "See how Python not only tells you the kind of error but where the error occurs? (Sometimes it will actually point out the exact spot in a line!)\n",
    "\n",
    "What we need to do is embed one of the tokenizers inside the other, stacking them as it were, in order to get the results we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for sentence in sent_tokenize(mdg):\n",
    "    for word in word_tokenize(sentence):\n",
    "        tokens.append(word)\n",
    "print(tokens[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a bit unfair that your second sight of a `for` loop should be one wrapped inside another, but this bit of code is not as complicated as it might look. Let's examine it more closely:\n",
    "\n",
    "**`tokens = []`** creates an empty list, like first putting a bucket under a spigot before turning it on. (If you think about it, the idea that you can create the bucket as you pour the water is both weird and cool, but here's the old-fashioned way of doing things. Hmm, it feels solid.)\n",
    "\n",
    "**`for sentence in sent_tokenize(mdg):`** grabs a sentence at a time using the power of the sentence tokenizer to do the work.\n",
    "\n",
    "**`for word in word_tokenize(sentence):`** tells Python, \"Hey, bud, while you've got that sentence in your hand, would you go ahead and tokenize it?\"\n",
    "\n",
    "**`tokens.append(word)`** drops each word, or punctuation!, into the `tokens` bucket. \n",
    "\n",
    "** **`print(tokens[0:50])`** just helps us check out work.\n",
    "\n",
    "**Nota bene**: about this constant `print` thing I do to check my work, feel free to drop it anywhere, especially in the middle of things like `for` loops: it's a great way to see what the program is doing and helps you learn how to code more quickly. (This is something my collaborator, Katherine Kinnaird, taught me.)\n",
    "\n",
    "Okay, that's enough for one workbook. We will return to this combination, or stack, of sentence and word tokenizers in the next workbook when we turn to the matter of counting words and creating dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've established how we are going to tokenize our strings into words, we can start counting those words, er, tokens! In the remaining part of this notebook, we are going to explore three ways to compile word frequencies for a text:\n",
    "\n",
    "- The Built-In Way\n",
    "- The NLTK Way\n",
    "- The Pandas way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# First we load our file into a string\n",
    "mdg = open('mdg.txt', 'r').read()\n",
    "\n",
    "# Then we turn that string into a list of words\n",
    "mdg_words = re.sub(\"[^a-zA-Z']\",\" \", mdg).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mdg_series = pd.Series(mdg_words)\n",
    "\n",
    "print(mdg_series[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_counts = mdg_series.value_counts()\n",
    "print(mdg_counts[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_counts.to_csv('mdg.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
