{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 08 Worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Working around VS Code\n",
    "\n",
    "I use VS Code to present materials in class because it presents the simplest / cleanest interface. Having said that, it is very frustrating that Code does not wrap text in the output window. The following code is a workaround and one you need not include in your notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# You will see lines that look like this:\n",
    "# wrapped_text = textwrap.fill(text, width=80) \n",
    "# print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading All the Things\n",
    "\n",
    "I like to load all the things I need at the beginning of my notebook. This way, I can see all the packages I am using and it makes it easier to find them later.\n",
    "\n",
    "After my imports, I define a custom function which takes a string and returns a list of tuples with the first element being the word (or token) and the second element being that word's part of speech tag.\n",
    "\n",
    "After that, I load my test file. \n",
    "\n",
    "I am using separate cells for each task so I can troubleshoot more readily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import nltk\n",
    "from pathlib import Path # only needed for complete corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM FUNCTION\n",
    "# Notice how there's documentation built into the function:\n",
    "def tagPOS (a_string):\n",
    "    \"\"\"\n",
    "    Takes a string and returns a list of tuples with the word \n",
    "    and its part of speech\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(a_string)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only am I loading data with this cell, but I am also the ability to \"slice\" a string by character index to begin to explore the text. I repeatedly enter various starting number with a 50-character limit above that until I get to some place where the text seems to start. Here I discovered there was a synopsis that starts at the 300 mark. \n",
    "\n",
    "NOTE: I came back later and decided to add `lower()` to the line where the file gets read. I realized that some adverbs (especially) might occur at the start of sentences, and I wanted to make sure that I was not missing any of those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  synopsis\n",
      "\n",
      "     en route back to earth from a far part of the galaxy, the crew of the\n",
      "\n",
      "     starship snark intercepts a transmission in an \n",
      "\n",
      "alien language\n",
      "\n",
      ",\n",
      "\n",
      "     originating from a\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DATA\n",
    "with open(\"../queue/scifi/alien.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            the_string = f.read().lower()\n",
    "\n",
    "print(the_string[300:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Getting the Parts-of-Speech\n",
    "\n",
    "I could probably place the custom function here, and normally I would, keeping the custom functions to where I will be using them in a notebook. (Composing a notebook is different than composing a script: notebooks are about you documenting your efforts; scripts are for well-developed code that is going to be used in something like production.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('en', 'IN')\n",
      "('route', 'NN')\n",
      "('back', 'RB')\n",
      "('to', 'TO')\n",
      "('earth', 'NN')\n",
      "('from', 'IN')\n",
      "('a', 'DT')\n",
      "('far', 'RB')\n",
      "('part', 'NN')\n",
      "('of', 'IN')\n"
     ]
    }
   ],
   "source": [
    "# Break them into a list of tokens\n",
    "tagged_text = tagPOS(the_string)\n",
    "\n",
    "for i in tagged_text[30:40]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27277"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am tempted to write another custom function to get that part of speech out of the list, but for now I am going to work my way through this exercise with simply for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['formerly', 'back', 'far', 'not', 'then', 'horribly', 'not', 'ultimately', 'however', 'now', 'then', 'finally', 'only', 'alone', 'only', 'broussard', 'along', 'eerily', 'sterile', 'softly', 'abruptly', 'close', 'gradually', 'slowly', 'groggily', 'now', 'triumphantly', 'not', 'fully', 'around', 'well', 'not', 'just', 'too', 'right', 'blackness', 'silently', 'back', 'only', \"n't\", 'broussard', 'just', \"n't\", 'even', 'yet', 'not', 'home', 'only', 'halfway', 'here']\n",
      "1282\n"
     ]
    }
   ],
   "source": [
    "# A for loop to work through our list of POS tuples\n",
    "# and retrieve only the words that match the POS we want:\n",
    "\n",
    "adverbs = []\n",
    "for i in tagged_text:\n",
    "    if i[1] == \"RB\":\n",
    "        adverbs.append(i[0])\n",
    "\n",
    "print(adverbs[0:50])\n",
    "print(len(adverbs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the adverbs in a list, I can now go through the list and count them. I am going to use a dictionary to keep track of the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank dictionary:\n",
    "adverb_counts = {}\n",
    "\n",
    "# Loop through the list of adverbs and count them:\n",
    "for i in adverbs:\n",
    "    if i in adverb_counts:\n",
    "        adverb_counts[i] += 1\n",
    "    else:\n",
    "        adverb_counts[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to create a new dictionary here. I don't know why. It's a habit as well as a strange phobia I have not to mess with established objects. Nothing should stop you from writing over the old dictionary.\n",
    "\n",
    "The ability to sort a dictionary is fairly recent (3.6+). All the  code says is: get the items and sort them by the second item in the tuple (the count) and then reverse the order so the highest count is first. (Default sort is ascending order, so the lowest number would be first.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"n't\", 122), ('then', 86), ('here', 57), ('back', 54), ('now', 51), ('just', 48), ('not', 43), ('there', 37), ('well', 29), ('right', 26)]\n"
     ]
    }
   ],
   "source": [
    "# Let's get that dictionary sorted:\n",
    "most_freq_adverbs = sorted(adverb_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(most_freq_adverbs[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, wow, those are some underwhelming adverbs. I am not sure what I was expecting, but I was hoping for something a little more exciting. Maybe I need to use a stop word list to get rid of the common adverbs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Common Adverbs\n",
    "\n",
    "What I am hoping to do here is use a list of common adverbs that I found [online](https://engdic.org/100-most-common-adverbs-list/), which I tweaked by removing adverbs that I thought might be interesting with regard to a collection of science fiction texts, and then I will use that revised list to filter out the common adverbs. The result, I hope, will be a list of adverbs that are more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about above absolutely actually after almost always anywhere around\n",
      "backward basically before below brightly broadly carefully certainly\n",
      "clearly closely completely daily definitely directly early enough\n",
      "especially even everywhere exactly firmly forward frequently generally\n",
      "here indirectly just kindly late likely loudly mainly meanwhile merely\n",
      "naturally nearly never notably nowhere obviously often partially\n",
      "particularly possibly pretty probably quickly quietly quite rarely\n",
      "rather really remarkably sadly sharply significantly simply slightly\n",
      "slowly so solely sometimes somewhat somewhere specifically suddenly\n",
      "there too totally universally upward usually utterly very warmly wholly\n",
      "widely \"n't\" then back now not well right again only still away maybe\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/adverbs.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    adverbs = f.read().split(', ')\n",
    "\n",
    "wrapped_text = textwrap.fill(' '.join(adverbs), width=72) \n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the question becomes: when in the series of steps above is it best to filter out the common adverbs? As I am splitting the string into a list? Filter the original list? Filter the dictionary?\n",
    "\n",
    "For reference, filtering the list would look like this:\n",
    "\n",
    "```python\n",
    "# Create a new list to hold the filtered words \n",
    "filtered_words = [] \n",
    "\n",
    "# Iterate over the list of words \n",
    "for word in words: \n",
    "  # If the word is not in the stop word list, add it to the filtered list \n",
    "  if word not in stop_words: \n",
    "    filtered_words.append(word) \n",
    "```\n",
    "I think I will try filtering the dictionary first. That seems to me the least computationally expensive. (But I don't really know.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dictionary comprehension\n",
    "# It could be written as a for loop, but this is more compact\n",
    "# \"if not in\" filters out the adverbs we don't want\n",
    "filtered = {key: adverb_counts[key] for key in adverb_counts if key not in adverbs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n",
      "(\"n't\", 122)\n",
      "('down', 26)\n",
      "('broussard', 18)\n",
      "('up', 14)\n",
      "('as', 13)\n",
      "('finally', 12)\n",
      "('instantly', 11)\n",
      "('yet', 10)\n",
      "('far', 8)\n",
      "('alone', 8)\n",
      "('close', 8)\n",
      "('faust', 8)\n",
      "('immediately', 8)\n",
      "('along', 6)\n",
      "('inside', 6)\n",
      "('abruptly', 5)\n",
      "('yes', 5)\n",
      "('out', 5)\n",
      "('ahead', 5)\n",
      "('nervously', 5)\n",
      "('frantically', 5)\n",
      "('long', 5)\n",
      "('silently', 4)\n",
      "('extremely', 4)\n",
      "('together', 4)\n",
      "('first', 4)\n",
      "('standard', 4)\n",
      "('fast', 4)\n",
      "('tightly', 4)\n",
      "('later', 4)\n",
      "('gradually', 3)\n",
      "('halfway', 3)\n",
      "('heavily', 3)\n",
      "('violently', 3)\n",
      "('i', 3)\n",
      "('else', 3)\n",
      "('barely', 3)\n",
      "('strangely', 3)\n",
      "('no', 3)\n",
      "('much', 3)\n",
      "('also', 3)\n",
      "('somehow', 3)\n",
      "('anyway', 3)\n",
      "('already', 3)\n",
      "('softly', 2)\n",
      "('atmosphere', 2)\n",
      "('hardly', 2)\n",
      "('soon', 2)\n",
      "('longer', 2)\n",
      "('okay', 2)\n"
     ]
    }
   ],
   "source": [
    "# Now we will need to sort the filtered dictionary:\n",
    "freq_adverbs = sorted(filtered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(len(freq_adverbs))\n",
    "for i in freq_adverbs[0:50]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I appear to be having difficult knocking out *n't* from my strings. *n't* is an artifact of the way NLTK's word tokenization works. If I used my backup tokenizer that uses regex, I would not have this problem. Since this is one small problem among many others, I am choosing to ignore it.\n",
    "\n",
    "To be honest, the adverbs for one screenplay are not terribly interesting. What if we did the entire corpus?\n",
    "\n",
    "And, honestly, I have no way to account for \"broussard\" being an adverb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Adverbs of the Entire Corpus\n",
    "\n",
    "We know how to load a bunch of files. For our purposes, we can merge all the screenplays into one big string, and then we can run the same code as above to get the adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "screenplays = []\n",
    "for p in Path('../queue/scifi/').glob('*.txt'):\n",
    "    with open(p, encoding=\"utf8\", errors='ignore') as f:\n",
    "        contents = f.read()\n",
    "        screenplays.append(contents)\n",
    "\n",
    "all_screenplays = ' '.join(screenplays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below combines the custom function with the for loop we created in the second section to get all the adverbs out of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdverbs (a_string):\n",
    "    \"\"\"\n",
    "    Takes a string and returns a list of tuples with the word \n",
    "    and its part of speech\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(a_string)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    adverbs = []\n",
    "    for i in tagged:\n",
    "        if i[1] == \"RB\":\n",
    "            adverbs.append(i[0])\n",
    "    return adverbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With custom function loaded, let's turn it loose on all our screenplays. \n",
    "\n",
    "I admit I wrote it as a custom function because I thought I would use it in a for loop to iterate over all the screen plays, but I realized that I could just combine all the screen plays into one giant text if all I care about is *all* the adverbs.\n",
    "\n",
    "That noted, it would be interested to run this as a for loop, keeping screenplays separate, and then running TF-IDF to see which adverbs were common to all screenplays and which ones distinguished one text, or group of texts, from the rest of the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176159\n"
     ]
    }
   ],
   "source": [
    "all_adverbs = getAdverbs(all_screenplays)\n",
    "print(len(all_adverbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not\n",
      "not\n",
      "enough\n",
      "Then\n",
      "not\n",
      "here\n",
      "Suddenly\n",
      "together\n",
      "finally\n",
      "apart\n",
      "no\n",
      "so\n",
      "back\n",
      "literally\n",
      "literally\n",
      "then\n",
      "then\n",
      "simply\n",
      "violently\n",
      "forward\n"
     ]
    }
   ],
   "source": [
    "# Let's see the first adverbs\n",
    "for adverb in all_adverbs[0:20]:\n",
    "    print(adverb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was a little surprised to see duplicates until I realized that this is a complete list, and not a list of counts, and so duplicates would be present. I also see that I did not lowercase the texts. *Sigh.* I will have to do this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85563\n"
     ]
    }
   ],
   "source": [
    "filtered = [word for word in all_adverbs if word not in adverbs]\n",
    "print(len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3489\n"
     ]
    }
   ],
   "source": [
    "# Create a blank dictionary:\n",
    "counts = {}\n",
    "\n",
    "# Loop through the list of adverbs and count them:\n",
    "for i in filtered:\n",
    "    if i in counts:\n",
    "        counts[i] += 1\n",
    "    else:\n",
    "        counts[i] = 1\n",
    "\n",
    "print(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"n't\", 19558)\n",
      "('down', 3590)\n",
      "('Then', 2959)\n",
      "('up', 2212)\n",
      "('Now', 1658)\n",
      "('Not', 1342)\n",
      "('Suddenly', 1227)\n",
      "('as', 1179)\n",
      "('ever', 1123)\n",
      "('far', 1080)\n",
      "('together', 998)\n",
      "('ahead', 949)\n",
      "('already', 895)\n",
      "('close', 847)\n",
      "('once', 776)\n",
      "('long', 763)\n",
      "('finally', 743)\n",
      "('alone', 739)\n",
      "('else', 724)\n",
      "('inside', 686)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now we will need to sort the filtered dictionary:\n",
    "counts_sorted = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Let's see the top 20 adverbs:\n",
    "for i in counts_sorted[0:20]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job done. You can stop looking at this notebook now. The next section is simply me exploring what kinds of views pandas might be able to offer us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3b: Lowercasing & Using Regex\n",
    "\n",
    "I will be curious to see if there's a speed difference between using NLTK's `word_tokenize` and using regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdverbs (a_string):\n",
    "    \"\"\"\n",
    "    Takes a string and returns a list of tuples with the word \n",
    "    and its part of speech\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(a_string)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    adverbs = []\n",
    "    for i in tagged:\n",
    "        if i[1] == \"RB\":\n",
    "            adverbs.append(i[0])\n",
    "    return adverbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: The Pandas Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(counts_sorted, columns=['adverb', 'count'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(counts,\n",
    "                            orient='index', \n",
    "                            columns=['count'])\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "370",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
